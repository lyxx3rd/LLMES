{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 3.31G/3.31G [00:11<00:00, 309MB/s] \n",
      "Downloading: 100%|██████████| 27.1k/27.1k [00:00<00:00, 52.1kB/s]\n",
      "Downloading: 100%|██████████| 5.85k/5.85k [00:00<00:00, 12.3kB/s]\n",
      "Downloading: 100%|██████████| 6.71M/6.71M [00:28<00:00, 244kB/s]\n",
      "Downloading: 100%|██████████| 7.13k/7.13k [00:00<00:00, 11.0kB/s]\n",
      "Downloading: 100%|██████████| 2.65M/2.65M [00:00<00:00, 4.84MB/s]\n"
     ]
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('qwen/Qwen2.5-7B-Instruct', cache_dir = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01256f8f3089455a9831f2fc01d000da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"./qwen/Qwen2___5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence (AI) model that has been trained on vast amounts of text data to understand and generate human-like language. These models can process, comprehend, and generate text in multiple languages, making them versatile tools for various natural language processing (NLP) tasks.\n",
      "\n",
      "Key characteristics of large language models include:\n",
      "\n",
      "1. **Scale**: They are typically very large, with billions or even trillions of parameters, which allows them to capture complex patterns and relationships within the text data they have been trained on.\n",
      "\n",
      "2. **Training Data**: These models are trained on extensive datasets that can include a wide range of texts from books, articles, websites, and more, which helps them learn about various topics and styles.\n",
      "\n",
      "3. **Capabilities**: Large language models can perform a variety of NLP tasks such as translation, summarization, question-answering, text completion, and more. They can also engage in conversations, provide creative writing suggestions, and offer insights based on the data they have learned.\n",
      "\n",
      "4. **Limitations**: While powerful, these models can sometimes generate incorrect information, exhibit biases present in their training data, and require significant computational resources for both training and inference.\n",
      "\n",
      "5. **Applications**: They find applications in areas like customer service chatbots, content creation, educational tools, and more, enhancing the ability of machines to understand and interact with humans using natural language.\n",
      "\n",
      "Large language models represent a significant advancement in AI technology and continue to evolve with ongoing research and development.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
